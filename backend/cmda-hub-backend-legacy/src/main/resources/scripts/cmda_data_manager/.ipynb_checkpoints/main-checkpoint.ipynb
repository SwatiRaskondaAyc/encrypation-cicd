{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated.\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nselib_wrapper import CapitalMarket\n",
    "from utils.db_handler import DBHandler\n",
    "\n",
    "from data_fetcher import MktDB\n",
    "\n",
    "from data_updater import HistPA_BackupDump_Updater\n",
    "from data_updater import HistPA_Updater\n",
    "from data_updater import DailyTimeFrame_Updater\n",
    "# from fact_updater.time_frame_updater import TimeFrameUpdater\n",
    "# from fact_updater.stats_updater import StatsUpdater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "conn, cursor = DBHandler.get_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkt_db = MktDB(conn, cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Always use 'YYYY-MM-DD' Date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for 18-01-2025:  Data not found, change the trade_date...\n",
      "Error fetching data for 19-01-2025: Data mismatch: Received data for 2025-01-17 00:00:00 instead of 19-01-2025\n",
      "Successfully received data for 20-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-20\n",
      "Successfully received data for 21-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-21\n",
      "Successfully received data for 22-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-22\n",
      "Successfully received data for 23-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-23\n",
      "Successfully received data for 24-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-24\n",
      "Error fetching data for 25-01-2025:  Data not found, change the trade_date...\n",
      "Error fetching data for 26-01-2025: Data mismatch: Received data for 2025-01-24 00:00:00 instead of 26-01-2025\n",
      "Successfully received data for 27-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-27\n",
      "Successfully received data for 28-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-28\n",
      "Successfully received data for 29-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-29\n",
      "Successfully received data for 30-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-30\n",
      "Successfully received data for 31-01-2025\n",
      "MktDB updated Successfully for date: 2025-01-31\n",
      "Successfully received data for 01-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-01\n",
      "Error fetching data for 02-02-2025: Data mismatch: Received data for 2025-02-01 00:00:00 instead of 02-02-2025\n",
      "Successfully received data for 03-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-03\n",
      "Successfully received data for 04-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-04\n",
      "Successfully received data for 05-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-05\n",
      "Successfully received data for 06-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-06\n",
      "Successfully received data for 07-02-2025\n",
      "MktDB updated Successfully for date: 2025-02-07\n",
      "Error fetching data for 08-02-2025:  Data not found, change the trade_date...\n"
     ]
    }
   ],
   "source": [
    "last_update_dt = '2025-01-17'\n",
    "last_update_dt = datetime.strptime(last_update_dt, \"%Y-%m-%d\").date()\n",
    "mkt_date = last_update_dt + dt.timedelta(days=1)\n",
    "\n",
    "today = dt.date.today()\n",
    "past_date = today - dt.timedelta(days=1)  # Previous day\n",
    "\n",
    "bulk_update(from_date=mkt_date, to_date=past_date, conn=conn, cursor=cursor)\n",
    "# # Symbol Changes in ListedSecurities table\n",
    "# ListedSecurities = mkt_db.get_listed_securities()\n",
    "# histPA_symbols = pd.read_sql(\"Select Distinct([Symbol]) from [dbo].[HistPA];\", conn)\n",
    "\n",
    "# # Symbols present in ListedSecurities and in HistPA\n",
    "# # Green Flag to send them in Further Table\n",
    "# available_histPA_symbol_isins = pd.merge(histPA_symbols, ListedSecurities, how='inner', on='Symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_update(from_date, to_date, conn, cursor):\n",
    "    while from_date <= to_date:\n",
    "        mkt_date_str = from_date.strftime('%Y-%m-%d')  # Requires 'YYYY-MM-DD' format to filter data                                                     \n",
    "        try:\n",
    "            # ---------------------> Fetch Data from Nse (nselib) <---------------------\n",
    "            mkt_bhav = CapitalMarket.get_market_bhav(from_date) # Requires either a date obj or date in 'DD-MM-YYYY' format\n",
    "            \n",
    "            # ---------------------> Update Table `HistPA_BackupDump` <---------------------\n",
    "            updater = HistPA_BackupDump_Updater(cursor)\n",
    "            updater.insert_data(mkt_bhav)\n",
    "            \n",
    "            # ---------------------> Filter mkt_bhav & Update Table `HistPA` <---------------------\n",
    "            # Filters Data so that each security will have only one Price Action record date \n",
    "            # This problem is caused becuase of Security trading on multiple series\n",
    "            filtered_mkt_bhav = CapitalMarket._filter_mkt_bhav(mkt_bhav)\n",
    "            updater = HistPA_Updater(cursor) \n",
    "            updater.insert_data(filtered_mkt_bhav)\n",
    "\n",
    "            \n",
    "            # ISINs and their info from table DailyTimeFrame\n",
    "            fetch_query = '''SELECT [ISIN], Min([Date]) as fetch_StartDate, Max([Date]) fetch_EndDate, count(*) records\n",
    "                          FROM [dbo].[DailyTimeFrame]\n",
    "                          GROUP BY [ISIN]'''\n",
    "            \n",
    "            dtf_isin_info = pd.read_sql(fetch_query, conn)\n",
    "            dtf_isin_info['fetch_EndDate'] = pd.to_datetime(dtf_isin_info['fetch_EndDate'])\n",
    "            dtf_isin_info['fetch_StartDate'] = pd.to_datetime(dtf_isin_info['fetch_EndDate'])\n",
    "            \n",
    "            out_of_update_isin = set(dtf_isin_info[dtf_isin_info['fetch_EndDate'] != last_update_dt]['ISIN'])\n",
    "            len(out_of_update_isin)\n",
    "            \n",
    "            # Securities that are dumped in HistPA but are not available in DailyTimeFrame\n",
    "            # --> In HistPA, ListedSec; but not in DailyTimeFrame\n",
    "            dtf_missing_isin_data = set(available_histPA_symbol_isins['ISIN']) - set(dtf_isin_info['ISIN'])\n",
    "            dtf_missing_isin_data = ListedSecurities[ListedSecurities['ISIN'].isin(dtf_missing_isin_data)] \n",
    "            len(dtf_missing_isin_data)\n",
    "            \n",
    "            # ISINs in DailyTimeFrame but now are missing in ListedSecurities\n",
    "            # Red Flag ---> The ISIN may change under a major change in corporate\n",
    "            set(dtf_isin_info['ISIN']) - set(available_histPA_symbol_isins['ISIN'])\n",
    "            \n",
    "            \n",
    "            filtered_mkt_bhav = pd.merge(ListedSecurities[['Symbol','ISIN']], filtered_mkt_bhav,  how='right', on='Symbol')\n",
    "            filtered_mkt_bhav.dropna(subset=['ISIN'], inplace=True)\n",
    "            set(dtf_isin_info['ISIN']) - set(filtered_mkt_bhav['ISIN'])\n",
    "            \n",
    "            # Only keep those records for which we have calculated data in the DailyTimeFrame Table\n",
    "            filtered_mkt_bhav = pd.merge(dtf_isin_info['ISIN'], filtered_mkt_bhav, how='inner', on='ISIN')\n",
    "            filtered_mkt_bhav = filtered_mkt_bhav[~filtered_mkt_bhav['ISIN'].isna()]\n",
    "            \n",
    "                    \n",
    "            # ---------------------> Update Table `DailyTimeFrame` <---------------------\n",
    "            updater = DailyTimeFrame_Updater(cursor, conn)\n",
    "            updater.insert_data(filtered_mkt_bhav, today_str=mkt_date_str)\n",
    "\n",
    "            print(f\"MktDB updated Successfully for date: {mkt_date_str}\")\n",
    "            from_date += dt.timedelta(days=1) \n",
    "            \n",
    "            conn.commit()\n",
    "        \n",
    "        except Exception as e:\n",
    "            from_date += dt.timedelta(days=1) # Increment even if error, to avoid infinite loop\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_mkt_bhav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolChange = pd.read_sql(\"SELECT * FROM SymbolChange;\", conn)\n",
    "\n",
    "# Symbol Changes in ListedSecurities table\n",
    "ListedSecurities = pd.read_sql(\"SELECT * FROM ListedSecurities\", conn)\n",
    "ListedSecurities['ListingDate'] = pd.to_datetime(ListedSecurities['ListingDate'], format='mixed')\n",
    "\n",
    "for index, row in symbolChange.iterrows():\n",
    "    update_symbol_query = f\"\"\"\n",
    "        UPDATE [dbo].[ListedSecurities] \n",
    "        SET Symbol = '{row['NewSymbol']}' \n",
    "        WHERE Symbol = '{row['PrevSymbol']}';\n",
    "    \"\"\"\n",
    "    # cursor.execute(update_symbol_query)\n",
    "\n",
    "\n",
    "# Symbol Changes in HistPA table\n",
    "histPA_symbols = pd.read_sql(\"Select Distinct([Symbol]) from [dbo].[HistPA];\", conn)\n",
    "symbol_changes = pd.merge(histPA_symbols, symbolChange, how='inner', left_on='Symbol', right_on='PrevSymbol')\n",
    "\n",
    "\n",
    "# Handle Symbol Changes in HistPA table\n",
    "for index, row in symbol_changes.iterrows():\n",
    "    update_symbol_query = f\"\"\"\n",
    "        UPDATE [dbo].[HistPA] \n",
    "        SET Symbol = '{row['NewSymbol']}' \n",
    "        WHERE Symbol = '{row['PrevSymbol']}';\n",
    "    \"\"\"\n",
    "    # print(update_symbol_query)\n",
    "    # cursor.execute(update_symbol_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Symbols not present in ListedSecurities but are present in HistPA\n",
    "# Red Flag to Update ListedSecurities\n",
    "na_sybmol_isins = pd.merge(histPA_symbols, ListedSecurities, how='left', on='Symbol')\n",
    "na_sybmol_isins = na_sybmol_isins[na_sybmol_isins['CompanyName'].isna()]\n",
    "len(na_sybmol_isins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbols present in ListedSecurities and in HistPA\n",
    "# Green Flag to send them in Further Table\n",
    "\n",
    "available_histPA_symbol_isins = pd.merge(histPA_symbols, ListedSecurities, how='inner', on='Symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Write Table `HistPA_BackupDump`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = HistPA_BackupDump_Updater(cursor)\n",
    "updater.insert_data(mkt_bhav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Write Table `HistPA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_mkt_bhav = CapitalMarket._filter_mkt_bhav(mkt_bhav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internally Filters Data so that each security will have only one Price Action record date \n",
    "# This problem is caused becuase of Security trading on multiple series\n",
    "updater = HistPA_Updater(cursor) \n",
    "updater.insert_data(filtered_mkt_bhav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISINs and their info from table DailyTimeFrame\n",
    "fetch_query = '''SELECT [ISIN], Min([Date]) as fetch_StartDate, Max([Date]) fetch_EndDate, count(*) records\n",
    "              FROM [dbo].[DailyTimeFrame]\n",
    "              GROUP BY [ISIN]'''\n",
    "\n",
    "dtf_isin_info = pd.read_sql(fetch_query, conn)\n",
    "dtf_isin_info['fetch_EndDate'] = pd.to_datetime(dtf_isin_info['fetch_EndDate'])\n",
    "dtf_isin_info['fetch_StartDate'] = pd.to_datetime(dtf_isin_info['fetch_EndDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_update_isin = set(dtf_isin_info[dtf_isin_info['fetch_EndDate'] != last_update_dt]['ISIN'])\n",
    "len(out_of_update_isin)\n",
    "\n",
    "# Securities that are dumped in HistPA but are not available in DailyTimeFrame\n",
    "# --> In HistPA, ListedSec; but not in DailyTimeFrame\n",
    "dtf_missing_isin_data = set(available_histPA_symbol_isins['ISIN']) - set(dtf_isin_info['ISIN'])\n",
    "dtf_missing_isin_data = ListedSecurities[ListedSecurities['ISIN'].isin(dtf_missing_isin_data)] \n",
    "len(dtf_missing_isin_data)\n",
    "\n",
    "# ISINs in DailyTimeFrame but now are missing in ListedSecurities\n",
    "# Red Flag ---> The ISIN may change under a major change in corporate\n",
    "set(dtf_isin_info['ISIN']) - set(available_histPA_symbol_isins['ISIN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INE326T01011', 'INE778K01012'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mkt_bhav = pd.merge(ListedSecurities[['Symbol','ISIN']], filtered_mkt_bhav,  how='right', on='Symbol')\n",
    "filtered_mkt_bhav.dropna(subset=['ISIN'], inplace=True)\n",
    "set(dtf_isin_info['ISIN']) - set(filtered_mkt_bhav['ISIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1745, 16)\n"
     ]
    }
   ],
   "source": [
    "# Only keep those records for which we have calculated data in the DailyTimeFrame Table\n",
    "filtered_mkt_bhav = pd.merge(dtf_isin_info['ISIN'], filtered_mkt_bhav, how='inner', on='ISIN')\n",
    "filtered_mkt_bhav = filtered_mkt_bhav[~filtered_mkt_bhav['ISIN'].isna()]\n",
    "print(filtered_mkt_bhav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Write Table `DailyTimeFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = DailyTimeFrame_Updater(cursor, conn)\n",
    "updater.insert_data(filtered_mkt_bhav, today_str='2025-01-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn.rollback()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicators(group):\n",
    "\n",
    "    group[\"PCT_Chng_D\"] = group[\"Close\"].pct_change(fill_method=None) * 100\n",
    "    group[\"PCT_Chng_W\"] = group[\"Close\"].pct_change(5, fill_method=None) * 100\n",
    "    group[\"PCT_Chng_M\"] = group[\"Close\"].pct_change(21, fill_method=None) * 100\n",
    "    group[\"PCT_Chng_Y\"] = group[\"Close\"].pct_change(260, fill_method=None) * 100\n",
    "    group[\"Spread\"] = group['Open'] - group['Close']\n",
    "\n",
    "    # Calculate daily price changes\n",
    "    delta = group['Close'].diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "\n",
    "    # Calculate the average gain and loss for RSI\n",
    "    window_length = 14\n",
    "    avg_gain = gain.rolling(window=window_length, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window_length, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    group['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Calculate the 12-day and 26-day EMAs for MACD\n",
    "    ema_12 = group['Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = group['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Calculate MACD and Signal Line\n",
    "    group['MACD'] = ema_12 - ema_26\n",
    "    group['Signal_Line'] = group['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Calculate Divergence\n",
    "    group['Divergence'] = (group['MACD'] - group['Signal_Line']).abs()\n",
    "\n",
    "    # Identify trend reversals\n",
    "    group['Trend_Reversal'] = np.where(\n",
    "        (group['MACD'] > group['Signal_Line']) & (group['MACD'].shift(1) <= group['Signal_Line']) |\n",
    "        (group['MACD'] < group['Signal_Line']) & (group['MACD'].shift(1) >= group['Signal_Line']),\n",
    "        True, False\n",
    "    )\n",
    "\n",
    "    # Identify continuous trends\n",
    "    lookback = 5\n",
    "    group['Continuous_Trend'] = ~group['Trend_Reversal'].rolling(window=lookback, center=True).max().fillna(0).astype(bool)\n",
    "\n",
    "    # Calculate thresholds for divergence\n",
    "    continuous_divergence = group[group['Continuous_Trend']]['Divergence']\n",
    "    group['Continuous_Trend_Mean_Divergence'] = continuous_divergence.mean()\n",
    "    group['Reversal_Mean_Divergence'] = group[group['Trend_Reversal']]['Divergence'].mean()\n",
    "    group['Threshold_Divergence'] = continuous_divergence.mean() + continuous_divergence.std()\n",
    "\n",
    "    # Mark crossover points\n",
    "    crossover_points = (group['MACD'] < group['Signal_Line']).shift() != (group['MACD'] < group['Signal_Line'])\n",
    "    group['Bullish_Crossover'] = crossover_points & (group['MACD'] > group['Signal_Line'])\n",
    "    group['Bearish_Crossover'] = crossover_points & (group['MACD'] < group['Signal_Line'])\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isin_list = filtered_mkt_bhav['ISIN'].tolist()\n",
    "\n",
    "fetch_query = f'''\n",
    "WITH RankedData AS (\n",
    "    SELECT *, \n",
    "        ROW_NUMBER() OVER (PARTITION BY ISIN ORDER BY [Date] DESC) AS RowNum \n",
    "    FROM [dbo].[DailyTimeFrame]\n",
    "    WHERE [ISIN] IN ({', '.join(['?'] * len(isin_list) )})\n",
    ")\n",
    "SELECT * \n",
    "FROM RankedData\n",
    "WHERE RowNum <= 270\n",
    "ORDER BY ISIN, [Date] ASC;\n",
    "'''\n",
    "\n",
    "DailyTimeFrame = pd.read_sql(fetch_query, con=conn, params=isin_list)\n",
    "DailyTimeFrame['Date'] = pd.to_datetime(DailyTimeFrame['Date'], format='mixed')\n",
    "\n",
    "        \n",
    "DailyTimeFrame = pd.concat([DailyTimeFrame.drop(columns='RowNum'),\n",
    "                            filtered_mkt_bhav.drop(columns='Symbol')])\n",
    "\n",
    "DailyTimeFrame.sort_values(by=['ISIN','Date'], inplace=True)\n",
    "\n",
    "# Apply calculations to each ISIN group\n",
    "calculated_data = DailyTimeFrame.groupby('ISIN').apply(calculate_indicators)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = calculated_data.loc[calculated_data['Date']==mkt_date_str]\\\n",
    "                         .fillna(0).drop(columns='Threshold_Divergence')\\\n",
    "                         .values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
